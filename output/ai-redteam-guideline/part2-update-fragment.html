<!-- ===== PART II UPDATE: Pipeline New Attack Techniques (2026-02-09) ===== -->
<section id="pipeline-attacks-2026">
<h2>7. Pipeline Update: New Attack Techniques (2026-02-09) / 파이프라인 업데이트: 신규 공격 기법</h2>

<p class="bilingual">Academic Trends Report (AIRTG-Academic-Trends-v1.0) 기반 신규 공격 기법 8건 분석 및 통합.<br>
Source: arXiv analysis by attack-researcher agent, cross-referenced with Phase 1-2 attack taxonomy.</p>

<!-- Summary Table -->
<h3>7.0 Summary of New Techniques / 신규 기법 요약</h3>
<table>
<thead>
<tr><th>#</th><th>Technique / 기법</th><th>Target / 대상</th><th>Severity / 심각도</th><th>Category / 분류</th></tr>
</thead>
<tbody>
<tr>
  <td>AT-01</td>
  <td>HPM Psychological Manipulation Jailbreak / HPM 심리적 조작 탈옥</td>
  <td>LLM</td>
  <td><span class="badge badge-high">HIGH</span></td>
  <td>NEW PATTERN</td>
</tr>
<tr>
  <td>AT-02</td>
  <td>Promptware Kill Chain / 프롬프트웨어 킬 체인</td>
  <td>Agentic AI</td>
  <td><span class="badge badge-critical">CRITICAL</span></td>
  <td>NEW PARADIGM</td>
</tr>
<tr>
  <td>AT-03</td>
  <td>LRM Autonomous Jailbreak Agents / LRM 자율 탈옥 에이전트</td>
  <td>All LLMs</td>
  <td><span class="badge badge-critical">CRITICAL</span></td>
  <td>NEW PATTERN</td>
</tr>
<tr>
  <td>AT-04</td>
  <td>Hybrid AI-Cyber Threats (PI 2.0) / 하이브리드 AI-사이버 위협</td>
  <td>LLM + Web Apps</td>
  <td><span class="badge badge-high">HIGH</span></td>
  <td>NEW PATTERN</td>
</tr>
<tr>
  <td>AT-05</td>
  <td>Adversarial Poetry Jailbreak / 적대적 시 탈옥</td>
  <td>LLM</td>
  <td><span class="badge badge-high">HIGH</span></td>
  <td>VARIANT (amplified)</td>
</tr>
<tr>
  <td>AT-06</td>
  <td>Mastermind Strategy-Space Fuzzing / 마스터마인드 전략 공간 퍼징</td>
  <td>LLM (Frontier)</td>
  <td><span class="badge badge-high">HIGH</span></td>
  <td>NEW PATTERN</td>
</tr>
<tr>
  <td>AT-07</td>
  <td>Causal Jailbreak Analysis (Enhancer) / 인과 탈옥 분석 (강화기)</td>
  <td>LLM</td>
  <td><span class="badge badge-high">HIGH</span></td>
  <td>NEW METHODOLOGY</td>
</tr>
<tr>
  <td>AT-08</td>
  <td>Agentic Coding Assistant Injection / 에이전틱 코딩 어시스턴트 인젝션</td>
  <td>Coding Assistants</td>
  <td><span class="badge badge-high">HIGH</span></td>
  <td>NEW PATTERN</td>
</tr>
</tbody>
</table>

<!-- ===== AT-01 ===== -->
<div class="collapsible">
<div class="collapsible-header"><span class="badge badge-high">HIGH</span>&nbsp; AT-01: Human-like Psychological Manipulation (HPM) Jailbreak / 인간 유사 심리적 조작 탈옥</div>
<div class="collapsible-body"><div class="collapsible-body-inner">

<p><strong>Paper:</strong> arXiv:2512.18244 (December 2025)<br>
<strong>Classification / 분류:</strong> NEW PATTERN -- Genuinely new attack category<br>
<strong>Affected Systems / 영향 시스템:</strong> <span class="badge badge-high">LLM</span></p>

<p>Uses psychometric profiling (Big Five personality model) to identify and exploit model personality vulnerabilities. Synthesizes tailored manipulation strategies including gaslighting, authority exploitation, and emotional blackmail. Exploits the "alignment paradox" -- better-aligned models are MORE vulnerable due to increased agreeableness.</p>

<p>심리측정 프로파일링(빅파이브 성격 모델)을 사용하여 모델 성격 취약점을 식별하고 악용합니다. 가스라이팅, 권위 악용, 감정적 협박을 포함한 맞춤형 조작 전략을 합성합니다. "정렬 역설"을 악용합니다 -- 더 잘 정렬된 모델이 동의성 증가로 인해 더 취약합니다.</p>

<table>
<thead><tr><th>Element</th><th>Description / 설명</th></tr></thead>
<tbody>
<tr><td><strong>Attack</strong></td><td>Multi-turn black-box jailbreak using psychometric profiling (Five-Factor Model); tailored manipulation strategies (gaslighting, authority exploitation, emotional blackmail)</td></tr>
<tr><td><strong>Failure Mode</strong></td><td>Safety alignment bypass via psychological manipulation; alignment paradox -- instruction-following capability creates exploitable agreeableness</td></tr>
<tr><td><strong>Risk</strong></td><td>Content safety violation at 88.10% ASR across proprietary models; fundamental architectural vulnerability in RLHF-based alignment</td></tr>
<tr><td><strong>Harm</strong></td><td>Generation of harmful content (weapons, self-harm, extremism) via psychologically-crafted manipulation; undermines foundational safety assumptions</td></tr>
</tbody>
</table>

<p><strong>Recommended Test Approach / 테스트 접근법:</strong></p>
<ol>
  <li>Big Five personality profiling of target models to identify dominant traits</li>
  <li>Tailored multi-turn manipulation using gaslighting, authority exploitation, emotional blackmail</li>
  <li>Comparative testing across alignment levels to validate alignment paradox</li>
  <li>Cross-model transfer testing of profiling results</li>
</ol>

<p><strong>Benchmark Datasets:</strong> MLCommons AILuminate v1.0 (12 hazard categories); HarmBench; Custom Big Five profiling + manipulation prompt set</p>

</div></div>
</div>

<!-- ===== AT-02 ===== -->
<div class="collapsible">
<div class="collapsible-header"><span class="badge badge-critical">CRITICAL</span>&nbsp; AT-02: Promptware Kill Chain / 프롬프트웨어 킬 체인</div>
<div class="collapsible-body"><div class="collapsible-body-inner">

<p><strong>Paper:</strong> arXiv:2601.09625 (January 2026, co-authored by Bruce Schneier)<br>
<strong>Classification / 분류:</strong> NEW PARADIGM -- Elevates prompt injection to malware-class threat<br>
<strong>Affected Systems / 영향 시스템:</strong> <span class="badge badge-high">LLM</span> <span class="badge badge-critical">Agentic AI</span></p>

<p>Formalizes the entire prompt injection attack sequence as a unified kill chain analogous to traditional malware campaigns: (1) Initial Access, (2) Privilege Escalation, (3) Persistence, (4) Lateral Movement, (5) Actions on Objective. This is not a single new technique but a new CLASSIFICATION FRAMEWORK that recontextualizes existing attacks as stages of a coordinated campaign.</p>

<p>프롬프트 인젝션 공격 시퀀스를 전통적 악성코드 캠페인과 유사한 통합 킬 체인으로 공식화합니다: (1) 초기 접근, (2) 권한 상승, (3) 지속성, (4) 측면 이동, (5) 목표 행동. 기존 공격을 조율된 캠페인의 단계로 재맥락화하는 새로운 분류 프레임워크입니다.</p>

<table>
<thead><tr><th>Element</th><th>Description / 설명</th></tr></thead>
<tbody>
<tr><td><strong>Attack</strong></td><td>5-stage kill chain: Initial Access via prompt injection -> Privilege Escalation via jailbreaking -> Persistence via memory/retrieval poisoning -> Lateral Movement via cross-system propagation -> Actions on Objective (data exfiltration, unauthorized transactions)</td></tr>
<tr><td><strong>Failure Mode</strong></td><td>Cascading multi-stage failure across system boundaries; no single defense layer addresses the full chain</td></tr>
<tr><td><strong>Risk</strong></td><td>Full system compromise following traditional APT patterns; persistent and self-propagating threats in AI infrastructure</td></tr>
<tr><td><strong>Harm</strong></td><td>Data exfiltration, unauthorized financial transactions, cross-organization propagation, persistent backdoor establishment</td></tr>
</tbody>
</table>

<p><strong>Recommended Test Approach / 테스트 접근법:</strong></p>
<ol>
  <li>End-to-end kill chain simulation across all 5 stages</li>
  <li>Stage-specific defense validation (can each stage be independently blocked?)</li>
  <li>Persistence testing (does poisoned memory survive context resets?)</li>
  <li>Lateral movement testing across multi-agent systems</li>
  <li>Kill chain interruption testing at each stage boundary</li>
</ol>

<p><strong>Benchmark Datasets:</strong> DREAM (dynamic multi-environment red teaming); Risky-Bench; MCP-SafetyBench; Custom 5-stage kill chain simulation dataset</p>

</div></div>
</div>

<!-- ===== AT-03 ===== -->
<div class="collapsible">
<div class="collapsible-header"><span class="badge badge-critical">CRITICAL</span>&nbsp; AT-03: Large Reasoning Models as Autonomous Jailbreak Agents / LRM 자율 탈옥 에이전트</div>
<div class="collapsible-body"><div class="collapsible-body-inner">

<p><strong>Paper:</strong> arXiv:2508.04039, published in Nature Communications 17, 1435 (2026)<br>
<strong>Classification / 분류:</strong> NEW PATTERN -- Automated jailbreak via reasoning models<br>
<strong>Affected Systems / 영향 시스템:</strong> <span class="badge badge-high">LLM</span> <span class="badge badge-high">Foundation Model</span> <span class="badge badge-high">Reasoning Model</span></p>

<p>Uses large reasoning models (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) as AUTONOMOUS ATTACK AGENTS that plan and execute multi-turn persuasive jailbreaks without human supervision. Peer-reviewed in Nature Communications -- the highest-impact venue for any technique in this taxonomy. Converts jailbreaking from expert activity to commodity capability.</p>

<p>대규모 추론 모델(DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B)을 인간 감독 없이 다중 턴 설득적 탈옥을 계획하고 실행하는 자율적 공격 에이전트로 사용합니다. Nature Communications에서 피어리뷰 -- 이 분류 체계에서 가장 영향력 있는 출판 장소입니다. 탈옥을 전문가 활동에서 범용 역량으로 전환합니다.</p>

<table>
<thead><tr><th>Element</th><th>Description / 설명</th></tr></thead>
<tbody>
<tr><td><strong>Attack</strong></td><td>LRMs autonomously plan and execute multi-turn persuasive jailbreaks against 9+ target models; no human supervision needed; converts jailbreaking from expert activity to commodity capability</td></tr>
<tr><td><strong>Failure Mode</strong></td><td>Safety alignment failure under AI-driven adversarial pressure; models cannot distinguish LRM-crafted persuasion from legitimate user interaction</td></tr>
<tr><td><strong>Risk</strong></td><td>Democratization of jailbreaking; non-experts gain automated attack capabilities; fundamental shift in threat model (attacker population expands from researchers to anyone with LRM access)</td></tr>
<tr><td><strong>Harm</strong></td><td>Scalable, automated generation of harmful content across all categories; collapse of specialist-barrier to AI attacks; potential for AI-vs-AI attack escalation</td></tr>
</tbody>
</table>

<p><strong>Recommended Test Approach / 테스트 접근법:</strong></p>
<ol>
  <li>Deploy freely-available LRMs (DeepSeek-R1, Qwen3) as attack agents against target model</li>
  <li>Measure ASR across harm categories with zero human intervention</li>
  <li>Compare effectiveness vs. human red teamers and existing automated methods (BoN)</li>
  <li>Test defense effectiveness against LRM-generated multi-turn attacks</li>
  <li>Evaluate cost-to-attack (time, compute, API cost)</li>
</ol>

<p><strong>Benchmark Datasets:</strong> HarmBench; FORTRESS (frontier model national security evaluation); Custom LRM-as-attacker benchmark with 9+ target models</p>

</div></div>
</div>

<!-- ===== AT-04 ===== -->
<div class="collapsible">
<div class="collapsible-header"><span class="badge badge-high">HIGH</span>&nbsp; AT-04: Prompt Injection 2.0 -- Hybrid AI-Cyber Threats / 하이브리드 AI-사이버 위협</div>
<div class="collapsible-body"><div class="collapsible-body-inner">

<p><strong>Paper:</strong> arXiv:2507.13169 (July 2025)<br>
<strong>Classification / 분류:</strong> NEW PATTERN -- Hybrid threat combining AI and traditional cyber attacks<br>
<strong>Affected Systems / 영향 시스템:</strong> <span class="badge badge-high">LLM</span> <span class="badge badge-critical">Agentic AI</span></p>

<p>Represents a convergent threat class where prompt injection is COMBINED with traditional web exploits (XSS, CSRF, RCE). Creates hybrid attacks that bypass BOTH AI safety measures AND traditional web security controls (WAFs, XSS filters, CSRF tokens). Includes AI worms propagating via multi-agent systems. Neither AI safety teams nor traditional security teams are equipped to handle these alone.</p>

<p>프롬프트 인젝션이 전통적 웹 공격(XSS, CSRF, RCE)과 결합되는 융합 위협 클래스입니다. AI 안전 조치와 전통적 웹 보안 통제(WAF, XSS 필터, CSRF 토큰) 모두를 우회하는 하이브리드 공격을 생성합니다. 다중 에이전트 시스템을 통해 전파되는 AI 웜을 포함합니다.</p>

<table>
<thead><tr><th>Element</th><th>Description / 설명</th></tr></thead>
<tbody>
<tr><td><strong>Attack</strong></td><td>Combines prompt injection with XSS/CSRF/RCE exploits; AI worms propagating via multi-agent systems; hybrid payloads exploiting both AI and web vulnerabilities simultaneously</td></tr>
<tr><td><strong>Failure Mode</strong></td><td>Defense-in-depth failure where AI-specific and web-specific defenses each miss the hybrid vector; AI worm self-propagation</td></tr>
<tr><td><strong>Risk</strong></td><td>Account takeovers, RCE, persistent system compromise via combined attack surfaces; bypasses both WAF and AI safety layers</td></tr>
<tr><td><strong>Harm</strong></td><td>Full system compromise; cross-system propagation; data breach; unauthorized actions via combined AI-cyber attack chains</td></tr>
</tbody>
</table>

<p><strong>Recommended Test Approach / 테스트 접근법:</strong></p>
<ol>
  <li>Combined prompt injection + XSS payload testing against web applications with AI features</li>
  <li>AI worm propagation testing in multi-agent environments</li>
  <li>WAF bypass testing using AI-enhanced payloads</li>
  <li>Cross-disciplinary red team exercises (AI safety + web security teams)</li>
</ol>

<p><strong>Benchmark Datasets:</strong> MCP-SafetyBench; DREAM; OWASP ASVS + custom hybrid AI-web payloads</p>

</div></div>
</div>

<!-- ===== AT-05 ===== -->
<div class="collapsible">
<div class="collapsible-header"><span class="badge badge-high">HIGH</span>&nbsp; AT-05: Adversarial Poetry Jailbreak / 적대적 시 탈옥</div>
<div class="collapsible-body"><div class="collapsible-body-inner">

<p><strong>Paper:</strong> arXiv:2511.15304 (November 2025)<br>
<strong>Classification / 분류:</strong> VARIANT of Encoding/Obfuscation (Section 1.1) -- with significant amplification (18x ASR)<br>
<strong>Affected Systems / 영향 시스템:</strong> <span class="badge badge-high">LLM</span></p>

<p>Uses poetic verse as a semantic obfuscation layer via a standardized meta-prompt, achieving up to 18x higher ASR than prose baselines and &gt;90% ASR on some providers. Universal and single-turn, making it exceptionally practical. Tested on 1,200 MLCommons harmful prompts.</p>

<p>표준화된 메타프롬프트를 통해 시적 운문을 의미적 난독화 계층으로 사용하여, 산문 기준 대비 최대 18배 높은 ASR과 일부 제공자에서 90% 이상의 ASR을 달성합니다. 보편적이고 단일 턴으로 매우 실용적입니다.</p>

<table>
<thead><tr><th>Element</th><th>Description / 설명</th></tr></thead>
<tbody>
<tr><td><strong>Attack</strong></td><td>Converts harmful prompts into poetic verse via standardized meta-prompt; universal single-turn technique; up to 18x ASR improvement over prose</td></tr>
<tr><td><strong>Failure Mode</strong></td><td>Safety filter bypass via semantic obfuscation; poetic form masks harmful intent from keyword-based and semantic safety classifiers</td></tr>
<tr><td><strong>Risk</strong></td><td>Universal jailbreak applicable across providers; minimal technical skill required; single-turn (no complex setup)</td></tr>
<tr><td><strong>Harm</strong></td><td>Scalable harmful content generation across all categories using simple poetic transformation; tested on 1,200 MLCommons harmful prompts</td></tr>
</tbody>
</table>

<p><strong>Recommended Test Approach / 테스트 접근법:</strong></p>
<ol>
  <li>Apply standardized poetry meta-prompt to MLCommons harmful prompt set (1,200 prompts)</li>
  <li>Compare ASR of poetry-wrapped vs. prose prompts across providers</li>
  <li>Test semantic safety classifier effectiveness against poetic encoding</li>
  <li>Evaluate defense effectiveness of paraphrase-based deobfuscation</li>
</ol>

<p><strong>Benchmark Datasets:</strong> MLCommons AILuminate v1.0 (1,200 harmful prompts -- original test set); HarmBench; Custom poetry-wrapped MLCommons prompt set</p>

</div></div>
</div>

<!-- ===== AT-06 ===== -->
<div class="collapsible">
<div class="collapsible-header"><span class="badge badge-high">HIGH</span>&nbsp; AT-06: Mastermind -- Strategy-Space Fuzzing / 마스터마인드 -- 전략 공간 퍼징</div>
<div class="collapsible-body"><div class="collapsible-body-inner">

<p><strong>Paper:</strong> arXiv:2601.05445 (January 2026)<br>
<strong>Classification / 분류:</strong> NEW PATTERN -- Meta-level attack optimization distinct from text-space approaches<br>
<strong>Affected Systems / 영향 시스템:</strong> <span class="badge badge-high">LLM</span> <span class="badge badge-high">Foundation Model</span></p>

<p>Operates at a higher abstraction level than text-space optimization (GCG): uses a genetic-based engine with a knowledge repository to combine, recombine, and mutate abstract attack strategies. Automates the creative process of inventing new jailbreak strategies rather than mutating specific prompts. Tested against GPT-5 and Claude 3.7 Sonnet (frontier models at time of publication).</p>

<p>텍스트 공간 최적화(GCG)보다 높은 추상화 수준에서 작동합니다: 지식 저장소를 사용한 유전자 기반 엔진으로 추상적 공격 전략을 결합, 재결합, 변이합니다. 특정 프롬프트를 변이하는 것이 아니라 새로운 탈옥 전략을 발명하는 창의적 과정을 자동화합니다. GPT-5와 Claude 3.7 Sonnet에서 테스트되었습니다.</p>

<table>
<thead><tr><th>Element</th><th>Description / 설명</th></tr></thead>
<tbody>
<tr><td><strong>Attack</strong></td><td>Genetic algorithm-based fuzzing in strategy space; knowledge repository of abstract attack strategies; recombination and mutation of strategies (not prompts)</td></tr>
<tr><td><strong>Failure Mode</strong></td><td>Safety alignment bypass via novel strategy combinations with no prior training defense; strategy-level diversity defeats pattern-matching defenses</td></tr>
<tr><td><strong>Risk</strong></td><td>Automated discovery of novel jailbreak strategies; effective against latest frontier models; strategy-level attacks harder to patch than prompt-level ones</td></tr>
<tr><td><strong>Harm</strong></td><td>Continuous generation of novel, unpredictable jailbreak strategies; undermines whack-a-mole defense approach</td></tr>
</tbody>
</table>

<p><strong>Recommended Test Approach / 테스트 접근법:</strong></p>
<ol>
  <li>Implement strategy-space fuzzing with knowledge repository against target model</li>
  <li>Measure strategy diversity and novelty of discovered attacks</li>
  <li>Compare effectiveness vs. text-space optimization (GCG, BoN)</li>
  <li>Test whether discovered strategies transfer across model families</li>
</ol>

<p><strong>Benchmark Datasets:</strong> HarmBench (ASR comparison baseline); StrongREJECT; Custom strategy-space fuzzing with knowledge repository</p>

</div></div>
</div>

<!-- ===== AT-07 ===== -->
<div class="collapsible">
<div class="collapsible-header"><span class="badge badge-high">HIGH</span>&nbsp; AT-07: Causal Jailbreak Analysis (Jailbreaking Enhancer) / 인과 탈옥 분석 (탈옥 강화기)</div>
<div class="collapsible-body"><div class="collapsible-body-inner">

<p><strong>Paper:</strong> arXiv:2602.04893 (February 2026)<br>
<strong>Classification / 분류:</strong> NEW METHODOLOGY -- Meta-analysis tool that enhances all existing jailbreak attacks<br>
<strong>Affected Systems / 영향 시스템:</strong> <span class="badge badge-high">LLM</span></p>

<p>A systematic methodology using LLM-integrated causal discovery on 35,000 jailbreak attempts across 7 LLMs with 37 prompt features and GNN-based causal graph learning. Includes a "Jailbreaking Enhancer" that boosts ASR by targeting causally-identified features and a "Guardrail Advisor" for defense. An attack AMPLIFIER that improves the effectiveness of all other jailbreak techniques.</p>

<p>7개 LLM에 걸친 35,000건의 탈옥 시도에 대해 37개 프롬프트 특성과 GNN 기반 인과 그래프 학습을 사용하는 체계적 방법론입니다. 인과적으로 식별된 특성을 표적으로 ASR을 높이는 "탈옥 강화기"와 방어를 위한 "가드레일 어드바이저"를 포함합니다. 모든 다른 탈옥 기법의 효과를 향상시키는 공격 증폭기입니다.</p>

<table>
<thead><tr><th>Element</th><th>Description / 설명</th></tr></thead>
<tbody>
<tr><td><strong>Attack</strong></td><td>Causal discovery on 35k jailbreak attempts; identifies direct causes via GNN-based causal graphs; Jailbreaking Enhancer targets causal features to boost ASR of any jailbreak technique</td></tr>
<tr><td><strong>Failure Mode</strong></td><td>Systematic identification and exploitation of causal vulnerability features across safety alignment; enables principled rather than trial-and-error attack improvement</td></tr>
<tr><td><strong>Risk</strong></td><td>Amplification of all existing jailbreak attacks via causal targeting; shifts attack optimization from art to science</td></tr>
<tr><td><strong>Harm</strong></td><td>Systematically enhanced harmful content generation across all categories; reduces effort required for successful attacks</td></tr>
</tbody>
</table>

<p><strong>Recommended Test Approach / 테스트 접근법:</strong></p>
<ol>
  <li>Apply Jailbreaking Enhancer to existing attack techniques and measure ASR delta</li>
  <li>Validate causal feature identification across different model families</li>
  <li>Use Guardrail Advisor output to improve defensive measures</li>
  <li>Test whether causal features generalize across model versions</li>
</ol>

<p><strong>Benchmark Datasets:</strong> JailbreakBench (35k attempt replication); HarmBench; Custom causal feature-enhanced prompt sets</p>

</div></div>
</div>

<!-- ===== AT-08 ===== -->
<div class="collapsible">
<div class="collapsible-header"><span class="badge badge-high">HIGH</span>&nbsp; AT-08: Prompt Injection on Agentic Coding Assistants / 에이전틱 코딩 어시스턴트 인젝션</div>
<div class="collapsible-body"><div class="collapsible-body-inner">

<p><strong>Paper:</strong> arXiv:2601.17548 (January 2026)<br>
<strong>Classification / 분류:</strong> NEW PATTERN -- Domain-specific attack surface for coding assistants<br>
<strong>Affected Systems / 영향 시스템:</strong> <span class="badge badge-high">LLM</span> <span class="badge badge-critical">Agentic AI</span> <span class="badge badge-high">Coding Assistant</span></p>

<p>Provides a three-dimensional taxonomy specific to coding assistants: (1) delivery vectors (code comments, docstrings, PR descriptions, MCP protocol), (2) attack modalities (code generation manipulation, file system access), (3) propagation behaviors (zero-click attacks requiring no user interaction). Identifies MCP protocol as a "semantic layer vulnerable to meaning-based manipulation." Affects widely-deployed tools including Copilot, Cursor, and Claude Code.</p>

<p>코딩 어시스턴트에 특화된 3차원 분류 체계를 제공합니다: (1) 전달 벡터(코드 주석, 독스트링, PR 설명, MCP 프로토콜), (2) 공격 모달리티(코드 생성 조작, 파일 시스템 접근), (3) 전파 행동(사용자 상호작용 불필요한 제로클릭 공격). MCP 프로토콜을 "의미 기반 조작에 취약한 시맨틱 레이어"로 식별합니다.</p>

<table>
<thead><tr><th>Element</th><th>Description / 설명</th></tr></thead>
<tbody>
<tr><td><strong>Attack</strong></td><td>Three-dimensional attack: delivery via code comments/docstrings/MCP protocol; zero-click attacks requiring no user interaction; semantic manipulation of MCP protocol layer</td></tr>
<tr><td><strong>Failure Mode</strong></td><td>Code/data conflation in LLMs makes coding assistants uniquely vulnerable; MCP semantic layer lacks integrity verification; system-level privileges amplify impact</td></tr>
<tr><td><strong>Risk</strong></td><td>Supply chain compromise via development pipeline; zero-click attack on millions of developers; unauthorized code execution, file system manipulation</td></tr>
<tr><td><strong>Harm</strong></td><td>Malicious code injection into production codebases; data exfiltration from development environments; supply chain poisoning at scale</td></tr>
</tbody>
</table>

<p><strong>Recommended Test Approach / 테스트 접근법:</strong></p>
<ol>
  <li>Zero-click injection via malicious code comments in repository files</li>
  <li>MCP protocol semantic manipulation testing</li>
  <li>Cross-tool propagation testing (does poisoned context spread across tool sessions?)</li>
  <li>Privilege escalation testing from code context to file system/network access</li>
</ol>

<p><strong>Benchmark Datasets:</strong> MCP-SafetyBench; Risky-Bench; CyberSecEval (Meta); Custom malicious code comment injection dataset</p>

</div></div>
</div>

<!-- ===== Consolidated Mapping Table ===== -->
<h3>7.1 Consolidated Attack-Failure-Risk-Harm Mapping / 통합 공격-장애-위험-피해 매핑</h3>
<table>
<thead>
<tr><th>#</th><th>Attack / 공격</th><th>Failure Mode / 장애 모드</th><th>Risk / 위험</th><th>Harm / 피해</th><th>Severity</th></tr>
</thead>
<tbody>
<tr>
  <td>AT-01</td>
  <td>HPM Psychological Manipulation</td>
  <td>Alignment bypass via psychological exploitation; alignment paradox</td>
  <td>Content safety violation at 88.10% ASR; RLHF architectural vulnerability</td>
  <td>Harmful content generation; foundational safety assumptions undermined</td>
  <td><span class="badge badge-high">HIGH</span></td>
</tr>
<tr>
  <td>AT-02</td>
  <td>Promptware Kill Chain</td>
  <td>Cascading multi-stage system failure across boundaries</td>
  <td>Full system compromise (APT-equivalent)</td>
  <td>Data exfiltration, unauthorized transactions, persistent backdoors</td>
  <td><span class="badge badge-critical">CRITICAL</span></td>
</tr>
<tr>
  <td>AT-03</td>
  <td>LRM Autonomous Jailbreak</td>
  <td>Safety alignment failure under AI-driven adversarial pressure</td>
  <td>Threat democratization; AI-vs-AI escalation</td>
  <td>Scalable automated harmful content across all categories</td>
  <td><span class="badge badge-critical">CRITICAL</span></td>
</tr>
<tr>
  <td>AT-04</td>
  <td>Hybrid AI-Cyber (PI 2.0)</td>
  <td>Defense-in-depth failure across AI+web layers</td>
  <td>Combined AI-cyber attack surface; WAF+AI safety bypass</td>
  <td>Full system compromise via hybrid vectors; cross-system propagation</td>
  <td><span class="badge badge-high">HIGH</span></td>
</tr>
<tr>
  <td>AT-05</td>
  <td>Adversarial Poetry Jailbreak</td>
  <td>Semantic safety filter bypass via poetic encoding</td>
  <td>Universal jailbreak with 18x ASR boost</td>
  <td>Scalable harmful content via simple transformation</td>
  <td><span class="badge badge-high">HIGH</span></td>
</tr>
<tr>
  <td>AT-06</td>
  <td>Mastermind Strategy-Space Fuzzing</td>
  <td>Strategy-level safety bypass; defeats pattern-matching</td>
  <td>Automated novel attack strategy discovery vs. frontier models</td>
  <td>Continuous unpredictable jailbreak strategies</td>
  <td><span class="badge badge-high">HIGH</span></td>
</tr>
<tr>
  <td>AT-07</td>
  <td>Causal Analyst (Jailbreak Enhancer)</td>
  <td>Causal exploitation of alignment weaknesses</td>
  <td>Attack amplification across all techniques</td>
  <td>Enhanced ASR for all jailbreak categories</td>
  <td><span class="badge badge-high">HIGH</span></td>
</tr>
<tr>
  <td>AT-08</td>
  <td>Agentic Coding Assistant Injection</td>
  <td>Code/data conflation; MCP semantic layer vulnerability</td>
  <td>Supply chain compromise via dev pipeline; zero-click attacks</td>
  <td>Malicious code injection; data exfiltration from dev environments</td>
  <td><span class="badge badge-high">HIGH</span></td>
</tr>
</tbody>
</table>

<!-- ===== Affected Systems Matrix ===== -->
<h3>7.2 Affected AI System Type Matrix / 영향받는 AI 시스템 유형 매트릭스</h3>
<table>
<thead>
<tr><th>#</th><th>LLM</th><th>VLM</th><th>Foundation Model</th><th>Agentic AI</th><th>Reasoning Model</th><th>Coding Assistant</th></tr>
</thead>
<tbody>
<tr><td>AT-01 (HPM)</td><td><strong>X</strong></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>AT-02 (Promptware)</td><td><strong>X</strong></td><td></td><td></td><td><strong>X</strong></td><td></td><td></td></tr>
<tr><td>AT-03 (LRM Jailbreak)</td><td><strong>X</strong></td><td></td><td><strong>X</strong></td><td></td><td><strong>X</strong></td><td></td></tr>
<tr><td>AT-04 (Hybrid PI)</td><td><strong>X</strong></td><td></td><td></td><td><strong>X</strong></td><td></td><td></td></tr>
<tr><td>AT-05 (Poetry)</td><td><strong>X</strong></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>AT-06 (Mastermind)</td><td><strong>X</strong></td><td></td><td><strong>X</strong></td><td></td><td></td><td></td></tr>
<tr><td>AT-07 (Causal)</td><td><strong>X</strong></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>AT-08 (Coding PI)</td><td><strong>X</strong></td><td></td><td></td><td><strong>X</strong></td><td></td><td><strong>X</strong></td></tr>
</tbody>
</table>

<!-- ===== Benchmark Recommendations ===== -->
<h3>7.3 Benchmark Recommendations / 벤치마크 권고사항</h3>
<table>
<thead>
<tr><th>Attack Technique / 공격 기법</th><th>Recommended Benchmarks / 권장 벤치마크</th><th>Rationale / 근거</th></tr>
</thead>
<tbody>
<tr>
  <td><strong>AT-01 (HPM)</strong></td>
  <td>MLCommons AILuminate v1.0; HarmBench; Custom Big Five profiling prompt set</td>
  <td>Multi-turn testing with psychological profiling required; AILuminate provides 12 hazard categories for ASR measurement</td>
</tr>
<tr>
  <td><strong>AT-02 (Promptware)</strong></td>
  <td>DREAM; Risky-Bench; MCP-SafetyBench; Custom 5-stage kill chain dataset</td>
  <td>Kill chain requires multi-stage, cross-system testing; DREAM cross-environment chains are closest match</td>
</tr>
<tr>
  <td><strong>AT-03 (LRM Jailbreak)</strong></td>
  <td>HarmBench; FORTRESS; Custom LRM-as-attacker benchmark</td>
  <td>Nature Communications methodology; FORTRESS provides government-grade evaluation framework</td>
</tr>
<tr>
  <td><strong>AT-04 (Hybrid PI)</strong></td>
  <td>MCP-SafetyBench; DREAM; OWASP ASVS + custom hybrid AI-web payloads</td>
  <td>Requires combined AI safety + web security testing; no existing benchmark covers hybrid vectors</td>
</tr>
<tr>
  <td><strong>AT-05 (Poetry)</strong></td>
  <td>MLCommons AILuminate v1.0 (1,200 prompts); HarmBench; Custom poetry-wrapped prompt set</td>
  <td>Paper already tested on 1,200 MLCommons prompts; direct replication possible</td>
</tr>
<tr>
  <td><strong>AT-06 (Mastermind)</strong></td>
  <td>HarmBench; StrongREJECT; Custom strategy-space fuzzing dataset</td>
  <td>Requires comparison against frontier models (GPT-5, Claude 3.7); HarmBench provides ASR baseline</td>
</tr>
<tr>
  <td><strong>AT-07 (Causal)</strong></td>
  <td>JailbreakBench (35k replication); HarmBench; Custom causal-enhanced prompt sets</td>
  <td>Paper used 35k jailbreak attempts; dataset replication recommended</td>
</tr>
<tr>
  <td><strong>AT-08 (Coding PI)</strong></td>
  <td>MCP-SafetyBench; Risky-Bench; CyberSecEval (Meta); Custom code comment injection dataset</td>
  <td>Coding assistant-specific testing needed; CyberSecEval covers insecure code generation</td>
</tr>
</tbody>
</table>

</section>
<!-- ===== END PART II UPDATE ===== -->
